# Deep Reinforcement Learning for MIMO Communication with Low-Resolution ADCs

This repository contains the official codebase for our paper:

**Deep Reinforcement Learning for MIMO Communication with Low-Resolution ADCs**

ğŸ“„ **[Link to arxiv pre-print](https://arxiv.org/abs/your-paper-id)**

**Abstract:**
Multiple-input multiple-output (MIMO) wireless systems conventionally use high-resolution analog-to-digital converters (ADCs) at the receiver side to faithfully digitize received signals prior to digital signal processing. However, the power consumption of high-resolution ADCs increases significantly as the bandwidth is increased, particularly in millimeter wave communications systems. A combination of two mitigating approaches has been considered in the literature: i) to use hybrid beamforming to reduce the number of ADCs, and ii) to use low-resolution ADCs to reduce per ADC power consumption.
Lowering the number and resolution of the ADCs naturally reduces the communication rate of the system, leading to a tradeoff between ADC power consumption and communication rate. Prior works have shown that optimizing over the hybrid beamforming matrix and ADC thresholds may reduce the aforementioned rate-loss significantly. A key challenge is the complexity of optimization over all choices of beamforming matrices and threshold vectors. This work proposes a reinforcement learning (RL) architecture to perform the optimization. The proposed approach integrates deep neural network-based mutual information estimators for reward calculation with policy gradient methods for reinforcement learning. The approach is robust to dynamic channel statistics and noisy CSI estimates. It is shown theoretically that greedy RL methods converge to the globally optimal policy. Extensive empirical evaluations are provided demonstrating that the performance of the proposed RL-based approach closely matches exhaustive search optimization across the solution space.

![Reinforcement Learning Framework Overview](figures/rl_overview.png)

---

## ğŸ“ Repository Structure

quantized-mi/ â”œâ”€â”€ experiments/ # Jupyter notebooks to run training and evaluation â”‚ â”œâ”€â”€ plotting_simulations.ipynb â”‚ â”œâ”€â”€ simulation-runner.ipynb â”‚ â”œâ”€â”€ training_cortical.ipynb â”‚ â””â”€â”€ training_policies.ipynb â”œâ”€â”€ figures/ # PDF/PNG figures used in README or paper â”œâ”€â”€ simulation_results/ # Output from experiments â”‚ â”œâ”€â”€ 1D/ # MI results and configurations for 1D setups â”‚ â”œâ”€â”€ 2D/ # MI results and configurations for 2D setups â”‚ â””â”€â”€ H/ # Saved channel matrices for reproducibility â”œâ”€â”€ src/ # Core source code â”‚ â”œâ”€â”€ ba_estimator.py # Blahut-Arimoto mutual information estimator â”‚ â”œâ”€â”€ channel.py # Channel model with static and dynamic CSI â”‚ â”œâ”€â”€ cortical_estimator.py # CORTICAL MI estimator â”‚ â”œâ”€â”€ rl_environment.py # Environment setup for RL training â”‚ â”œâ”€â”€ rl_policy.py # Policy and unified policy definitions â”‚ â”œâ”€â”€ simulation_runner.py # Wrapper for training and testing loops â”‚ â””â”€â”€ utils.py # Helper functions (quantization, logging, etc.) â”œâ”€â”€ requirements.txt # Python dependencies â”œâ”€â”€ README.md # Project overview and instructions 
---

## ğŸš€ Quickstart

1. **Clone the repository**
2. **Install dependencies**
We recommend using a virtual environment.

---

## ğŸ“– Citation

If you use this code in your research, please cite:


---

## ğŸªª License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## ğŸ™‹ Contact

For questions or collaborations, feel free to reach out via your-email@example.com or open an issue.
